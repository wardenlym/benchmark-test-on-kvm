I0319 05:52:33.928392 1729288 clusterloader.go:137] ClusterConfig.Nodes set to 4
I0319 05:52:33.928553 1729288 clusterloader.go:247] Using config: {ClusterConfig:{KubeConfigPath:/home/ubuntu/.kube/config RunFromCluster:false Nodes:4 Provider:local EtcdCertificatePath:/home/ubuntu/etcd/ca.crt EtcdKeyPath:/home/ubuntu/etcd/ca.key EtcdInsecurePort:2382 MasterIPs:[172.29.50.31] MasterInternalIPs:[172.29.50.31] MasterName:master-01 KubemarkRootKubeConfigPath: DeleteStaleNamespaces:false DeleteAutomanagedNamespaces:true SSHToMasterSupported:true APIServerPprofByClientEnabled:true KubeletPort:10250} ReportDir:./mode-3-10-30-reports-2021-03-19-0552 EnableExecService:true TestScenario:{Identifier: ConfigPath: OverridePaths:[]} ModifierConfig:{OverwriteTestConfig:[] SkipSteps:[]} PrometheusConfig:{EnableServer:false TearDownServer:true ScrapeEtcd:false ScrapeNodeExporter:false ScrapeKubelets:false ScrapeKubeProxy:true ScrapeAnet:false SnapshotProject: ManifestPath:$GOPATH/src/k8s.io/perf-tests/clusterloader2/pkg/prometheus/manifests CoreManifests:$GOPATH/src/k8s.io/perf-tests/clusterloader2/pkg/prometheus/manifests/*.yaml DefaultServiceMonitors:$GOPATH/src/k8s.io/perf-tests/clusterloader2/pkg/prometheus/manifests/default/*.yaml MasterIPServiceMonitors:$GOPATH/src/k8s.io/perf-tests/clusterloader2/pkg/prometheus/manifests/master-ip/*.yaml NodeExporterPod:$GOPATH/src/k8s.io/perf-tests/clusterloader2/pkg/prometheus/manifests/exporters/node-exporter.yaml}}
I0319 05:52:33.938670 1729288 cluster.go:58] Listing cluster nodes:
I0319 05:52:33.938708 1729288 cluster.go:70] Name: master-01, clusterIP: 172.29.50.31, externalIP: , isSchedulable: false
I0319 05:52:33.938719 1729288 cluster.go:70] Name: master-02, clusterIP: 172.29.50.32, externalIP: , isSchedulable: false
I0319 05:52:33.938726 1729288 cluster.go:70] Name: master-03, clusterIP: 172.29.50.33, externalIP: , isSchedulable: false
I0319 05:52:33.938732 1729288 cluster.go:70] Name: worker-01, clusterIP: 172.29.50.41, externalIP: , isSchedulable: true
I0319 05:52:33.938738 1729288 cluster.go:70] Name: worker-02, clusterIP: 172.29.50.42, externalIP: , isSchedulable: true
I0319 05:52:33.938743 1729288 cluster.go:70] Name: worker-03, clusterIP: 172.29.50.43, externalIP: , isSchedulable: true
I0319 05:52:33.938749 1729288 cluster.go:70] Name: worker-04, clusterIP: 172.29.50.44, externalIP: , isSchedulable: true
I0319 05:52:33.947457 1729288 exec_service.go:62] Exec service: setting up service!
I0319 05:52:33.980922 1729288 framework.go:239] Applying pkg/execservice/manifest/exec_deployment.yaml
I0319 05:52:34.013045 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(cluster-loader), labelSelector(feature = exec)
I0319 05:52:34.013298 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(cluster-loader), labelSelector(feature = exec)
I0319 05:52:44.064908 1729288 wait_for_pods.go:90] Exec service: namespace(cluster-loader), labelSelector(feature = exec): Pods: 3 out of 3 created, 3 running (3 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:52:44.065249 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(cluster-loader), labelSelector(feature = exec)
I0319 05:52:44.065614 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(cluster-loader), labelSelector(feature = exec)
I0319 05:52:44.065687 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(cluster-loader), labelSelector(feature = exec)
I0319 05:52:44.116436 1729288 exec_service.go:99] Exec service: service set up successfully!
W0319 05:52:44.116733 1729288 imagepreload.go:86] No images specified. Skipping image preloading
I0319 05:52:44.116831 1729288 clusterloader.go:208] --------------------------------------------------------------------------------
I0319 05:52:44.116860 1729288 clusterloader.go:209] Running ./mode-3-10-30-reports-2021-03-19-0552/config.yaml
I0319 05:52:44.116870 1729288 clusterloader.go:210] --------------------------------------------------------------------------------
I0319 05:52:44.123499 1729288 simple_test_executor.go:56] AutomanagedNamespacePrefix: test-irdrhg
I0319 05:52:44.154356 1729288 simple_test_executor.go:139] Step "[step: 01] Starting measurements" started
I0319 05:52:44.154760 1729288 etcd_metrics.go:86] EtcdMetrics: starting etcd metrics collecting...
W0319 05:52:44.154801 1729288 prometheus_measurement.go:59] APIResponsivenessPrometheus: Prometheus is disabled, skipping the measurement!
W0319 05:52:44.154805 1729288 prometheus_measurement.go:59] APIResponsivenessPrometheusSimple: Prometheus is disabled, skipping the measurement!
W0319 05:52:44.154877 1729288 probes.go:106] InClusterNetworkLatency: Prometheus is disabled, skipping the measurement!
W0319 05:52:44.155100 1729288 probes.go:106] DnsLookupLatency: Prometheus is disabled, skipping the measurement!
I0319 05:52:44.173348 1729288 scheduler_latency.go:128] SchedulingMetrics: start collecting latency initial metrics in scheduler...
I0319 05:52:44.204537 1729288 resource_usage.go:112] ResourceUsageSummary: starting resource usage collecting...
I0319 05:52:44.232971 1729288 profile.go:273] Exposing kube-apiserver debug endpoint for anonymous access
I0319 05:52:44.251614 1729288 profile.go:273] Exposing kube-apiserver debug endpoint for anonymous access
I0319 05:52:44.264005 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-01/kube-scheduler usage on node master-01. CPUUsageInCores: 0.005659759, MemoryUsageInBytes: 89276416, MemoryWorkingSetInBytes: 41963520
I0319 05:52:44.264048 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-ttms9/coredns usage on node master-01. CPUUsageInCores: 0.00538518, MemoryUsageInBytes: 33345536, MemoryWorkingSetInBytes: 10637312
I0319 05:52:44.264058 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-01/kube-controller-manager usage on node master-01. CPUUsageInCores: 0.003269766, MemoryUsageInBytes: 56381440, MemoryWorkingSetInBytes: 25542656
I0319 05:52:44.264067 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-01/kube-apiserver usage on node master-01. CPUUsageInCores: 0.135646385, MemoryUsageInBytes: 999088128, MemoryWorkingSetInBytes: 567619584
I0319 05:52:44.264076 1729288 resource_gather_worker.go:68] Get container kube-proxy-wf64w/kube-proxy usage on node master-01. CPUUsageInCores: 0.000100896, MemoryUsageInBytes: 54063104, MemoryWorkingSetInBytes: 27795456
I0319 05:52:44.264085 1729288 resource_gather_worker.go:68] Get container etcd-master-01/etcd usage on node master-01. CPUUsageInCores: 0.064672987, MemoryUsageInBytes: 650059776, MemoryWorkingSetInBytes: 161759232
I0319 05:52:44.264093 1729288 resource_gather_worker.go:68] Get container calico-node-jp8v5/calico-node usage on node master-01. CPUUsageInCores: 0.042246042, MemoryUsageInBytes: 71581696, MemoryWorkingSetInBytes: 57024512
I0319 05:52:44.269590 1729288 system_pod_metrics.go:125] collecting system pod metrics...
I0319 05:52:44.294247 1729288 system_pod_metrics.go:229] Loaded restart count threshold overrides: map[]
I0319 05:52:44.294279 1729288 ooms_tracker.go:86] skipping tracking of OOMs in the cluster
I0319 05:52:44.294298 1729288 simple_test_executor.go:165] Step "[step: 01] Starting measurements" ended
I0319 05:52:44.294314 1729288 simple_test_executor.go:139] Step "[step: 02] Starting saturation pod measurements" started
I0319 05:52:44.294385 1729288 pod_startup_latency.go:110] PodStartupLatency: labelSelector(group = saturation): starting pod startup latency measurement...
I0319 05:52:44.294462 1729288 wait_for_controlled_pods.go:163] WaitForControlledPodsRunning: starting wait for controlled pods measurement...
I0319 05:52:44.294590 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: labelSelector(group = saturation)
I0319 05:52:44.294603 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: labelSelector(group = saturation)
I0319 05:52:44.294779 1729288 reflector.go:175] Starting reflector <unspecified> (0s) from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:52:44.294879 1729288 reflector.go:211] Listing and watching <unspecified> from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:52:44.294858 1729288 reflector.go:175] Starting reflector *unstructured.Unstructured (0s) from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:52:44.294995 1729288 reflector.go:211] Listing and watching *unstructured.Unstructured from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:52:44.345384 1729288 scheduling_throughput.go:121] SchedulingThroughput: starting collecting throughput data
I0319 05:52:44.394633 1729288 simple_test_executor.go:165] Step "[step: 02] Starting saturation pod measurements" ended
I0319 05:52:44.394691 1729288 simple_test_executor.go:139] Step "[step: 03] Creating saturation pods" started
I0319 05:52:44.414864 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0)
I0319 05:52:44.414964 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0)
I0319 05:52:44.597941 1729288 simple_test_executor.go:165] Step "[step: 03] Creating saturation pods" ended
I0319 05:52:44.597996 1729288 simple_test_executor.go:139] Step "[step: 04] Waiting for saturation pods to be running" started
I0319 05:52:44.598086 1729288 wait_for_controlled_pods.go:188] WaitForControlledPodsRunning: waiting for controlled pods measurement...
I0319 05:52:49.345688 1729288 scheduling_throughput.go:136] SchedulingThroughput: labelSelector(group = saturation): 30 pods scheduled
I0319 05:52:49.467535 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0): Pods: 30 out of 30 created, 17 running (17 updated), 13 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:52:52.831900 1729288 resource_gather_worker.go:68] Get container etcd-master-02/etcd usage on node master-02. CPUUsageInCores: 0.161049858, MemoryUsageInBytes: 704200704, MemoryWorkingSetInBytes: 132972544
I0319 05:52:52.832182 1729288 resource_gather_worker.go:68] Get container calico-node-2dd2z/calico-node usage on node master-02. CPUUsageInCores: 0.048336307, MemoryUsageInBytes: 69267456, MemoryWorkingSetInBytes: 54128640
I0319 05:52:52.832240 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-02/kube-scheduler usage on node master-02. CPUUsageInCores: 0.004491058, MemoryUsageInBytes: 76029952, MemoryWorkingSetInBytes: 24637440
I0319 05:52:52.832292 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-02/kube-controller-manager usage on node master-02. CPUUsageInCores: 0.002238662, MemoryUsageInBytes: 48291840, MemoryWorkingSetInBytes: 25018368
I0319 05:52:52.832340 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-02/kube-apiserver usage on node master-02. CPUUsageInCores: 0.160857929, MemoryUsageInBytes: 1107517440, MemoryWorkingSetInBytes: 558657536
I0319 05:52:52.832388 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-44j7w/coredns usage on node master-02. CPUUsageInCores: 0.004710407, MemoryUsageInBytes: 32944128, MemoryWorkingSetInBytes: 9695232
I0319 05:52:52.832423 1729288 resource_gather_worker.go:68] Get container kube-proxy-tj5hq/kube-proxy usage on node master-02. CPUUsageInCores: 0.007112212, MemoryUsageInBytes: 53456896, MemoryWorkingSetInBytes: 25784320
I0319 05:52:54.313523 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-01/kube-apiserver usage on node master-01. CPUUsageInCores: 0.165070473, MemoryUsageInBytes: 999096320, MemoryWorkingSetInBytes: 567627776
I0319 05:52:54.313601 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-ttms9/coredns usage on node master-01. CPUUsageInCores: 0.004507073, MemoryUsageInBytes: 33345536, MemoryWorkingSetInBytes: 10637312
I0319 05:52:54.313613 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-01/kube-controller-manager usage on node master-01. CPUUsageInCores: 0.003269766, MemoryUsageInBytes: 56381440, MemoryWorkingSetInBytes: 25542656
I0319 05:52:54.313653 1729288 resource_gather_worker.go:68] Get container kube-proxy-wf64w/kube-proxy usage on node master-01. CPUUsageInCores: 0.007632661, MemoryUsageInBytes: 54050816, MemoryWorkingSetInBytes: 27783168
I0319 05:52:54.313671 1729288 resource_gather_worker.go:68] Get container etcd-master-01/etcd usage on node master-01. CPUUsageInCores: 0.126359651, MemoryUsageInBytes: 651292672, MemoryWorkingSetInBytes: 162045952
I0319 05:52:54.313681 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-01/kube-scheduler usage on node master-01. CPUUsageInCores: 0.016399878, MemoryUsageInBytes: 89276416, MemoryWorkingSetInBytes: 41963520
I0319 05:52:54.313694 1729288 resource_gather_worker.go:68] Get container calico-node-jp8v5/calico-node usage on node master-01. CPUUsageInCores: 0.056226333, MemoryUsageInBytes: 71602176, MemoryWorkingSetInBytes: 57180160
I0319 05:52:54.346943 1729288 scheduling_throughput.go:136] SchedulingThroughput: labelSelector(group = saturation): 30 pods scheduled
I0319 05:52:54.470315 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0): Pods: 30 out of 30 created, 30 running (30 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:52:54.470369 1729288 wait_for_controlled_pods.go:246] WaitForControlledPodsRunning: running 1, deleted 0, timeout: 0, unknown: 0
I0319 05:52:54.470392 1729288 wait_for_controlled_pods.go:260] WaitForControlledPodsRunning: 1/1 Deployments are running with all pods
I0319 05:52:54.470448 1729288 simple_test_executor.go:165] Step "[step: 04] Waiting for saturation pods to be running" ended
I0319 05:52:54.470472 1729288 simple_test_executor.go:139] Step "[step: 05] Collecting saturation pod measurements" started
I0319 05:52:54.470539 1729288 scheduling_throughput.go:149] SchedulingThroughput: gathering data
I0319 05:52:54.470751 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0)
I0319 05:52:54.470747 1729288 pod_startup_latency.go:130] PodStartupLatency: labelSelector(group = saturation): gathering pod startup latency measurement...
I0319 05:52:54.470912 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: labelSelector(group = saturation)
I0319 05:52:54.471019 1729288 reflector.go:181] Stopping reflector <unspecified> (0s) from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:52:54.488441 1729288 phase_latency.go:130] PodStartupLatency: 30 worst run_to_watch latencies: [{test-irdrhg-1/saturation-deployment-0-7b988b4b7f-pvxw9 157.522268ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-bvbrs 181.013462ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5fsrp 609.383276ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2stq9 633.508505ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2zxdl 857.971146ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4jr65 944.893483ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-6xw2k 982.68387ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-zr94r 1.091971221s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-h7l97 1.102852985s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vz4vn 1.126361162s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-mjzqn 1.141073193s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-wn666 1.300651856s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9dlkh 1.500845942s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-jj75j 1.698642982s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vb4g5 1.873509843s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xjwtz 1.898904161s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-fnmg8 1.905706958s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-gmxbg 1.917277941s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5wgdv 2.092956844s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-8m68t 2.109018938s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vhcg2 2.300309265s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-t6kbd 2.49327678s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-f5gjt 2.498869655s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-kwk64 2.70189662s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9fnb5 2.703749358s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4hnl6 2.896398105s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-dpn7f 3.097732677s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xldl8 3.304268507s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-crsmb 3.501049091s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2lt72 3.903863905s}]
I0319 05:52:54.488677 1729288 phase_latency.go:135] PodStartupLatency: perc50: 1.873509843s, perc90: 3.097732677s, perc99: 3.903863905s
I0319 05:52:54.488779 1729288 phase_latency.go:130] PodStartupLatency: 30 worst schedule_to_watch latencies: [{test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5fsrp 2.917131131s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2stq9 3.086409472s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2zxdl 3.265540502s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-zr94r 3.540689837s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-f5gjt 4.015718615s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-gmxbg 4.323868043s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vb4g5 4.330039061s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xjwtz 4.359365836s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4jr65 4.391603817s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-6xw2k 4.438760144s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-h7l97 4.541139912s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vz4vn 4.582588817s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-pvxw9 4.617718224s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-bvbrs 4.630116752s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-mjzqn 4.657618226s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xldl8 4.752437794s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-wn666 4.807908589s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-jj75j 4.957434903s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9dlkh 5.005401732s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9fnb5 5.212909183s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-fnmg8 5.311072971s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5wgdv 5.535926202s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-dpn7f 5.550933132s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vhcg2 5.749718687s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-crsmb 5.907052359s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-t6kbd 6.011349795s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-kwk64 6.113074981s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2lt72 6.29580824s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4hnl6 6.394512026s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-8m68t 6.471637604s}]
I0319 05:52:54.488933 1729288 phase_latency.go:135] PodStartupLatency: perc50: 4.657618226s, perc90: 6.113074981s, perc99: 6.471637604s
I0319 05:52:54.489036 1729288 phase_latency.go:130] PodStartupLatency: 30 worst pod_startup latencies: [{test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5fsrp 3.609383276s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2stq9 3.633508505s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2zxdl 3.857971146s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-zr94r 4.091971221s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-f5gjt 4.498869655s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vb4g5 4.873509843s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xjwtz 4.898904161s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-gmxbg 4.917277941s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4jr65 4.944893483s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-6xw2k 4.98268387s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-h7l97 5.102852985s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vz4vn 5.126361162s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-mjzqn 5.141073193s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-pvxw9 5.157522268s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-bvbrs 5.181013462s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-wn666 5.300651856s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xldl8 5.304268507s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9dlkh 5.500845942s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-jj75j 5.698642982s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9fnb5 5.703749358s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-fnmg8 5.905706958s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5wgdv 6.092956844s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-dpn7f 6.097732677s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vhcg2 6.300309265s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-t6kbd 6.49327678s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-crsmb 6.501049091s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-kwk64 6.70189662s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4hnl6 6.896398105s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2lt72 6.903863905s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-8m68t 7.109018938s}]
I0319 05:52:54.489182 1729288 phase_latency.go:135] PodStartupLatency: perc50: 5.181013462s, perc90: 6.70189662s, perc99: 7.109018938s; threshold 3m1s
I0319 05:52:54.489260 1729288 phase_latency.go:130] PodStartupLatency: 30 worst create_to_schedule latencies: [{test-irdrhg-1/saturation-deployment-0-7b988b4b7f-t6kbd 481.926985ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-f5gjt 483.15104ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-mjzqn 483.454967ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9fnb5 490.840175ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-wn666 492.743267ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9dlkh 495.44421ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4hnl6 501.886079ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xjwtz 539.538325ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-pvxw9 539.804044ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vb4g5 543.470782ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vz4vn 543.772345ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-6xw2k 543.923726ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-dpn7f 546.799545ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2stq9 547.099033ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vhcg2 550.590578ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-bvbrs 550.89671ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-zr94r 551.281384ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xldl8 551.830713ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4jr65 553.289666ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5wgdv 557.030642ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-h7l97 561.713073ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-kwk64 588.821639ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2zxdl 592.430644ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-gmxbg 593.409898ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-crsmb 593.996732ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-fnmg8 594.633987ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2lt72 608.055665ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-8m68t 637.381334ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5fsrp 692.252145ms} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-jj75j 741.208079ms}]
I0319 05:52:54.489379 1729288 phase_latency.go:135] PodStartupLatency: perc50: 550.590578ms, perc90: 608.055665ms, perc99: 741.208079ms
I0319 05:52:54.489454 1729288 phase_latency.go:130] PodStartupLatency: 30 worst schedule_to_run latencies: [{test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xldl8 1.448169287s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-f5gjt 1.51684896s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5fsrp 2.307747855s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2lt72 2.391944335s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-crsmb 2.406003268s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-gmxbg 2.406590102s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2zxdl 2.407569356s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-zr94r 2.448718616s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-2stq9 2.452900967s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-dpn7f 2.453200455s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vb4g5 2.456529218s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-xjwtz 2.460461675s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9fnb5 2.509159825s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-jj75j 3.258791921s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-fnmg8 3.405366013s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-kwk64 3.411178361s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-h7l97 3.438286927s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-5wgdv 3.442969358s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4jr65 3.446710334s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vhcg2 3.449409422s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-6xw2k 3.456076274s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-vz4vn 3.456227655s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-4hnl6 3.498113921s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-9dlkh 3.50455579s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-wn666 3.507256733s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-mjzqn 3.516545033s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-t6kbd 3.518073015s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-8m68t 4.362618666s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-bvbrs 4.44910329s} {test-irdrhg-1/saturation-deployment-0-7b988b4b7f-pvxw9 4.460195956s}]
I0319 05:52:54.489584 1729288 phase_latency.go:135] PodStartupLatency: perc50: 3.405366013s, perc90: 3.518073015s, perc99: 4.460195956s
I0319 05:52:54.489974 1729288 simple_test_executor.go:165] Step "[step: 05] Collecting saturation pod measurements" ended
I0319 05:52:54.490088 1729288 simple_test_executor.go:139] Step "[step: 06] Starting latency pod measurements" started
I0319 05:52:54.490210 1729288 wait_for_controlled_pods.go:163] WaitForControlledPodsRunning: starting wait for controlled pods measurement...
I0319 05:52:54.490309 1729288 pod_startup_latency.go:110] PodStartupLatency: labelSelector(group = latency): starting pod startup latency measurement...
I0319 05:52:54.490450 1729288 reflector.go:175] Starting reflector *unstructured.Unstructured (0s) from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:52:54.490503 1729288 reflector.go:211] Listing and watching *unstructured.Unstructured from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:52:54.490469 1729288 reflector.go:175] Starting reflector <unspecified> (0s) from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:52:54.490631 1729288 reflector.go:211] Listing and watching <unspecified> from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:52:54.591441 1729288 simple_test_executor.go:165] Step "[step: 06] Starting latency pod measurements" ended
I0319 05:52:54.591556 1729288 simple_test_executor.go:139] Step "[step: 07] Creating latency pods" started
I0319 05:52:54.611421 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-0)
I0319 05:52:54.611444 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-0)
I0319 05:52:54.809480 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-1)
I0319 05:52:54.809502 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-1)
I0319 05:52:55.004664 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-2)
I0319 05:52:55.004709 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-2)
I0319 05:52:55.206938 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-3)
I0319 05:52:55.207038 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-3)
I0319 05:52:55.415129 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-4)
I0319 05:52:55.415219 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-4)
I0319 05:52:55.615498 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-5)
I0319 05:52:55.615549 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-5)
I0319 05:52:55.807736 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-6)
I0319 05:52:55.807810 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-6)
I0319 05:52:56.011017 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-7)
I0319 05:52:56.011046 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-7)
I0319 05:52:56.215404 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-8)
I0319 05:52:56.215452 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-8)
I0319 05:52:56.412813 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-9)
I0319 05:52:56.412868 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-9)
I0319 05:52:56.616884 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-10)
I0319 05:52:56.616959 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-10)
I0319 05:52:56.813703 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-11)
I0319 05:52:56.813770 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-11)
I0319 05:52:57.011998 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-12)
I0319 05:52:57.012033 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-12)
I0319 05:52:57.220279 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-13)
I0319 05:52:57.220319 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-13)
I0319 05:52:57.420596 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-14)
I0319 05:52:57.420647 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-14)
I0319 05:52:57.612549 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-15)
I0319 05:52:57.612600 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-15)
I0319 05:52:57.812732 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-16)
I0319 05:52:57.812815 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-16)
I0319 05:52:58.018102 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-17)
I0319 05:52:58.018147 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-17)
I0319 05:52:58.218975 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-18)
I0319 05:52:58.219307 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-18)
I0319 05:52:58.414757 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-19)
I0319 05:52:58.414803 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-19)
I0319 05:52:58.618408 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20)
I0319 05:52:58.618455 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20)
I0319 05:52:58.820381 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-21)
I0319 05:52:58.820420 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-21)
I0319 05:52:59.018121 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22)
I0319 05:52:59.018180 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22)
I0319 05:52:59.220040 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-23)
I0319 05:52:59.220074 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-23)
I0319 05:52:59.422090 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24)
I0319 05:52:59.422122 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24)
I0319 05:52:59.624259 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25)
I0319 05:52:59.624291 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25)
I0319 05:52:59.662733 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-0): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:52:59.663085 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-0)
I0319 05:52:59.825273 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26)
I0319 05:52:59.825323 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26)
I0319 05:52:59.861021 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-1): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:52:59.861269 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-1)
I0319 05:53:00.022637 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27)
I0319 05:53:00.022662 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27)
I0319 05:53:00.055923 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-2): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:00.056488 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-2)
I0319 05:53:00.238273 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-28)
I0319 05:53:00.238299 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-28)
I0319 05:53:00.258069 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-3): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:00.258237 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-3)
I0319 05:53:00.426000 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29)
I0319 05:53:00.426035 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29)
I0319 05:53:00.466459 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-4): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:00.466807 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-4)
I0319 05:53:00.613257 1729288 simple_test_executor.go:165] Step "[step: 07] Creating latency pods" ended
I0319 05:53:00.613317 1729288 simple_test_executor.go:139] Step "[step: 08] Waiting for latency pods to be running" started
I0319 05:53:00.613365 1729288 wait_for_controlled_pods.go:188] WaitForControlledPodsRunning: waiting for controlled pods measurement...
I0319 05:53:00.666381 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-5): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:00.666552 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-5)
I0319 05:53:00.859116 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-6): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:00.859351 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-6)
I0319 05:53:01.061789 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-7): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:01.061999 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-7)
I0319 05:53:01.269595 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-8): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:01.269803 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-8)
I0319 05:53:01.393352 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-03/kube-apiserver usage on node master-03. CPUUsageInCores: 0.23882491, MemoryUsageInBytes: 1306689536, MemoryWorkingSetInBytes: 641445888
I0319 05:53:01.393426 1729288 resource_gather_worker.go:68] Get container etcd-master-03/etcd usage on node master-03. CPUUsageInCores: 0.214799116, MemoryUsageInBytes: 693407744, MemoryWorkingSetInBytes: 183943168
I0319 05:53:01.393438 1729288 resource_gather_worker.go:68] Get container calico-node-kl7lj/calico-node usage on node master-03. CPUUsageInCores: 0.053301123, MemoryUsageInBytes: 68780032, MemoryWorkingSetInBytes: 56799232
I0319 05:53:01.393450 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-03/kube-controller-manager usage on node master-03. CPUUsageInCores: 0.078594137, MemoryUsageInBytes: 184307712, MemoryWorkingSetInBytes: 109617152
I0319 05:53:01.393464 1729288 resource_gather_worker.go:68] Get container kube-proxy-lksgf/kube-proxy usage on node master-03. CPUUsageInCores: 0.009482043, MemoryUsageInBytes: 54366208, MemoryWorkingSetInBytes: 27299840
I0319 05:53:01.393474 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-f2d6s/coredns usage on node master-03. CPUUsageInCores: 0.00529826, MemoryUsageInBytes: 25329664, MemoryWorkingSetInBytes: 10596352
I0319 05:53:01.393508 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-03/kube-scheduler usage on node master-03. CPUUsageInCores: 0.013762706, MemoryUsageInBytes: 77541376, MemoryWorkingSetInBytes: 38236160
I0319 05:53:01.464419 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-9): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:01.464589 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-9)
I0319 05:53:01.668453 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-10): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:01.668904 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-10)
I0319 05:53:01.864679 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-11): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:01.864797 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-11)
I0319 05:53:02.062780 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-12): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:02.062983 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-12)
I0319 05:53:02.271140 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-13): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:02.271301 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-13)
I0319 05:53:02.472127 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-14): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:02.472453 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-14)
I0319 05:53:02.664031 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-15): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:02.664208 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-15)
I0319 05:53:02.858978 1729288 resource_gather_worker.go:68] Get container kube-proxy-tj5hq/kube-proxy usage on node master-02. CPUUsageInCores: 0.000165757, MemoryUsageInBytes: 53456896, MemoryWorkingSetInBytes: 25784320
I0319 05:53:02.859025 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-02/kube-apiserver usage on node master-02. CPUUsageInCores: 0.186276493, MemoryUsageInBytes: 1107517440, MemoryWorkingSetInBytes: 558657536
I0319 05:53:02.859040 1729288 resource_gather_worker.go:68] Get container etcd-master-02/etcd usage on node master-02. CPUUsageInCores: 0.221308468, MemoryUsageInBytes: 705597440, MemoryWorkingSetInBytes: 133017600
I0319 05:53:02.859051 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-02/kube-controller-manager usage on node master-02. CPUUsageInCores: 0.002790279, MemoryUsageInBytes: 48291840, MemoryWorkingSetInBytes: 25018368
I0319 05:53:02.859065 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-02/kube-scheduler usage on node master-02. CPUUsageInCores: 0.013113404, MemoryUsageInBytes: 76029952, MemoryWorkingSetInBytes: 24637440
I0319 05:53:02.859075 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-44j7w/coredns usage on node master-02. CPUUsageInCores: 0.004240301, MemoryUsageInBytes: 32944128, MemoryWorkingSetInBytes: 9695232
I0319 05:53:02.859085 1729288 resource_gather_worker.go:68] Get container calico-node-2dd2z/calico-node usage on node master-02. CPUUsageInCores: 0.052635906, MemoryUsageInBytes: 69267456, MemoryWorkingSetInBytes: 54128640
I0319 05:53:02.864239 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-16): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:02.864441 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-16)
I0319 05:53:03.068963 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-17): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:03.069216 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-17)
I0319 05:53:03.270276 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-18): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:03.270520 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-18)
I0319 05:53:03.467079 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-19): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:03.467306 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-19)
I0319 05:53:03.669006 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:03.669204 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20)
I0319 05:53:03.871197 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-21): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:03.871460 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-21)
I0319 05:53:04.069282 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:04.069507 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22)
I0319 05:53:04.271947 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-23): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:04.272302 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-23)
I0319 05:53:04.339620 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-01/kube-scheduler usage on node master-01. CPUUsageInCores: 0.025145316, MemoryUsageInBytes: 89276416, MemoryWorkingSetInBytes: 41963520
I0319 05:53:04.339658 1729288 resource_gather_worker.go:68] Get container calico-node-jp8v5/calico-node usage on node master-01. CPUUsageInCores: 0.056226333, MemoryUsageInBytes: 71602176, MemoryWorkingSetInBytes: 57180160
I0319 05:53:04.339665 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-ttms9/coredns usage on node master-01. CPUUsageInCores: 0.003715267, MemoryUsageInBytes: 33345536, MemoryWorkingSetInBytes: 10637312
I0319 05:53:04.339672 1729288 resource_gather_worker.go:68] Get container kube-proxy-wf64w/kube-proxy usage on node master-01. CPUUsageInCores: 0.007632661, MemoryUsageInBytes: 54050816, MemoryWorkingSetInBytes: 27783168
I0319 05:53:04.339678 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-01/kube-controller-manager usage on node master-01. CPUUsageInCores: 0.006128428, MemoryUsageInBytes: 56381440, MemoryWorkingSetInBytes: 25542656
I0319 05:53:04.339685 1729288 resource_gather_worker.go:68] Get container etcd-master-01/etcd usage on node master-01. CPUUsageInCores: 0.126359651, MemoryUsageInBytes: 651292672, MemoryWorkingSetInBytes: 162045952
I0319 05:53:04.339692 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-01/kube-apiserver usage on node master-01. CPUUsageInCores: 0.165070473, MemoryUsageInBytes: 999096320, MemoryWorkingSetInBytes: 567627776
I0319 05:53:04.473951 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:04.474191 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24)
I0319 05:53:04.675439 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:04.675627 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25)
I0319 05:53:04.876781 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26): Pods: 1 out of 1 created, 0 running (0 updated), 1 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:05.074261 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:05.074525 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27)
I0319 05:53:05.289666 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-28): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:05.289990 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-28)
I0319 05:53:05.477627 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:05.477779 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29)
I0319 05:53:09.877559 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26): Pods: 1 out of 1 created, 1 running (1 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:09.877640 1729288 wait_for_controlled_pods.go:246] WaitForControlledPodsRunning: running 30, deleted 0, timeout: 0, unknown: 0
I0319 05:53:09.877667 1729288 wait_for_controlled_pods.go:260] WaitForControlledPodsRunning: 30/30 Deployments are running with all pods
I0319 05:53:09.877688 1729288 simple_test_executor.go:165] Step "[step: 08] Waiting for latency pods to be running" ended
I0319 05:53:09.877710 1729288 simple_test_executor.go:139] Step "[step: 09] Deleting latency pods" started
I0319 05:53:09.877832 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26)
I0319 05:53:09.898420 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-0)
I0319 05:53:09.898536 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-0)
I0319 05:53:10.027882 1729288 resource_gather_worker.go:68] Get container kube-proxy-tbvfc/kube-proxy usage on node worker-01. CPUUsageInCores: 0.00901724, MemoryUsageInBytes: 55607296, MemoryWorkingSetInBytes: 26505216
I0319 05:53:10.027934 1729288 resource_gather_worker.go:68] Get container calico-node-zbwxj/calico-node usage on node worker-01. CPUUsageInCores: 0.097060849, MemoryUsageInBytes: 87752704, MemoryWorkingSetInBytes: 69025792
I0319 05:53:10.097221 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-1)
I0319 05:53:10.097253 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-1)
I0319 05:53:10.294131 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-2)
I0319 05:53:10.294152 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-2)
I0319 05:53:10.495092 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-3)
I0319 05:53:10.495215 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-3)
I0319 05:53:10.695882 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-4)
I0319 05:53:10.695910 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-4)
I0319 05:53:10.898511 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-5)
I0319 05:53:10.898585 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-5)
I0319 05:53:11.103205 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-6)
I0319 05:53:11.103244 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-6)
I0319 05:53:11.295215 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-7)
I0319 05:53:11.295368 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-7)
I0319 05:53:11.417160 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-f2d6s/coredns usage on node master-03. CPUUsageInCores: 0.00529826, MemoryUsageInBytes: 25329664, MemoryWorkingSetInBytes: 10596352
I0319 05:53:11.417323 1729288 resource_gather_worker.go:68] Get container calico-node-kl7lj/calico-node usage on node master-03. CPUUsageInCores: 0.054237699, MemoryUsageInBytes: 68780032, MemoryWorkingSetInBytes: 56664064
I0319 05:53:11.417431 1729288 resource_gather_worker.go:68] Get container kube-proxy-lksgf/kube-proxy usage on node master-03. CPUUsageInCores: 0.000349553, MemoryUsageInBytes: 54366208, MemoryWorkingSetInBytes: 27299840
I0319 05:53:11.417518 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-03/kube-apiserver usage on node master-03. CPUUsageInCores: 0.23882491, MemoryUsageInBytes: 1306689536, MemoryWorkingSetInBytes: 641445888
I0319 05:53:11.417605 1729288 resource_gather_worker.go:68] Get container etcd-master-03/etcd usage on node master-03. CPUUsageInCores: 0.214799116, MemoryUsageInBytes: 693407744, MemoryWorkingSetInBytes: 183943168
I0319 05:53:11.417654 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-03/kube-scheduler usage on node master-03. CPUUsageInCores: 0.013762706, MemoryUsageInBytes: 77541376, MemoryWorkingSetInBytes: 38236160
I0319 05:53:11.417751 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-03/kube-controller-manager usage on node master-03. CPUUsageInCores: 0.078594137, MemoryUsageInBytes: 184307712, MemoryWorkingSetInBytes: 109617152
I0319 05:53:11.498816 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-8)
I0319 05:53:11.498998 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-8)
I0319 05:53:11.705764 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-9)
I0319 05:53:11.705791 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-9)
I0319 05:53:11.899101 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-10)
I0319 05:53:11.899133 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-10)
I0319 05:53:12.098650 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-11)
I0319 05:53:12.098717 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-11)
I0319 05:53:12.305553 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-12)
I0319 05:53:12.305586 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-12)
I0319 05:53:12.506004 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-13)
I0319 05:53:12.506062 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-13)
I0319 05:53:12.702154 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-14)
I0319 05:53:12.702232 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-14)
I0319 05:53:12.881184 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-02/kube-apiserver usage on node master-02. CPUUsageInCores: 0.186276493, MemoryUsageInBytes: 1107517440, MemoryWorkingSetInBytes: 558657536
I0319 05:53:12.881255 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-02/kube-scheduler usage on node master-02. CPUUsageInCores: 0.016992618, MemoryUsageInBytes: 76029952, MemoryWorkingSetInBytes: 24637440
I0319 05:53:12.881268 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-44j7w/coredns usage on node master-02. CPUUsageInCores: 0.003662164, MemoryUsageInBytes: 32944128, MemoryWorkingSetInBytes: 9695232
I0319 05:53:12.881278 1729288 resource_gather_worker.go:68] Get container etcd-master-02/etcd usage on node master-02. CPUUsageInCores: 0.221308468, MemoryUsageInBytes: 705597440, MemoryWorkingSetInBytes: 133017600
I0319 05:53:12.881288 1729288 resource_gather_worker.go:68] Get container calico-node-2dd2z/calico-node usage on node master-02. CPUUsageInCores: 0.052635906, MemoryUsageInBytes: 69267456, MemoryWorkingSetInBytes: 54128640
I0319 05:53:12.881298 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-02/kube-controller-manager usage on node master-02. CPUUsageInCores: 0.002384095, MemoryUsageInBytes: 48291840, MemoryWorkingSetInBytes: 25018368
I0319 05:53:12.881308 1729288 resource_gather_worker.go:68] Get container kube-proxy-tj5hq/kube-proxy usage on node master-02. CPUUsageInCores: 9.2966e-05, MemoryUsageInBytes: 53456896, MemoryWorkingSetInBytes: 25784320
I0319 05:53:12.902955 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-15)
I0319 05:53:12.903089 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-15)
I0319 05:53:13.103599 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-16)
I0319 05:53:13.103656 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-16)
I0319 05:53:13.301305 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-17)
I0319 05:53:13.301367 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-17)
I0319 05:53:13.505659 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-18)
I0319 05:53:13.505691 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-18)
I0319 05:53:13.711365 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-19)
I0319 05:53:13.711467 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-19)
I0319 05:53:13.910554 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20)
I0319 05:53:13.910613 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20)
I0319 05:53:14.112057 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-21)
I0319 05:53:14.112128 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-21)
I0319 05:53:14.301739 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22)
I0319 05:53:14.301775 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22)
I0319 05:53:14.360815 1729288 resource_gather_worker.go:68] Get container kube-proxy-wf64w/kube-proxy usage on node master-01. CPUUsageInCores: 7.3751e-05, MemoryUsageInBytes: 54050816, MemoryWorkingSetInBytes: 27783168
I0319 05:53:14.360855 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-01/kube-apiserver usage on node master-01. CPUUsageInCores: 0.196637368, MemoryUsageInBytes: 999104512, MemoryWorkingSetInBytes: 567635968
I0319 05:53:14.360861 1729288 resource_gather_worker.go:68] Get container etcd-master-01/etcd usage on node master-01. CPUUsageInCores: 0.156272605, MemoryUsageInBytes: 652484608, MemoryWorkingSetInBytes: 163643392
I0319 05:53:14.360866 1729288 resource_gather_worker.go:68] Get container calico-node-jp8v5/calico-node usage on node master-01. CPUUsageInCores: 0.04616231, MemoryUsageInBytes: 71610368, MemoryWorkingSetInBytes: 57053184
I0319 05:53:14.360871 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-01/kube-scheduler usage on node master-01. CPUUsageInCores: 0.025145316, MemoryUsageInBytes: 89276416, MemoryWorkingSetInBytes: 41963520
I0319 05:53:14.360876 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-ttms9/coredns usage on node master-01. CPUUsageInCores: 0.003852421, MemoryUsageInBytes: 33345536, MemoryWorkingSetInBytes: 10637312
I0319 05:53:14.360880 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-01/kube-controller-manager usage on node master-01. CPUUsageInCores: 0.002371779, MemoryUsageInBytes: 56381440, MemoryWorkingSetInBytes: 25542656
I0319 05:53:14.507961 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-23)
I0319 05:53:14.508069 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-23)
I0319 05:53:14.712492 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24)
I0319 05:53:14.712562 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24)
I0319 05:53:14.908199 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25)
I0319 05:53:14.908238 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25)
I0319 05:53:14.950233 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-0): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:15.111039 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26)
I0319 05:53:15.111095 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26)
I0319 05:53:15.147827 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-1): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:15.308245 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27)
I0319 05:53:15.308278 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27)
I0319 05:53:15.345636 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-2): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:15.510908 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-28)
I0319 05:53:15.511004 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-28)
I0319 05:53:15.546992 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-3): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:15.547154 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-3)
I0319 05:53:15.709438 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29)
I0319 05:53:15.709476 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29)
I0319 05:53:15.747365 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-4): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:15.898296 1729288 simple_test_executor.go:165] Step "[step: 09] Deleting latency pods" ended
I0319 05:53:15.898367 1729288 simple_test_executor.go:139] Step "[step: 10] Waiting for latency pods to be deleted" started
I0319 05:53:15.898432 1729288 wait_for_controlled_pods.go:188] WaitForControlledPodsRunning: waiting for controlled pods measurement...
I0319 05:53:15.949634 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-5): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:16.154756 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-6): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:16.345985 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-7): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:16.346411 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-7)
I0319 05:53:16.550705 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-8): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:16.550921 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-8)
I0319 05:53:16.757490 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-9): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:16.757975 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-9)
I0319 05:53:16.951217 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-10): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:17.149849 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-11): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:17.356540 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-12): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:17.556850 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-13): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:17.752498 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-14): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:17.752694 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-14)
I0319 05:53:17.954361 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-15): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:17.954581 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-15)
I0319 05:53:18.154803 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-16): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:18.155071 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-16)
I0319 05:53:18.352846 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-17): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:18.557692 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-18): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:18.603602 1729288 resource_gather_worker.go:68] Get container kube-proxy-nww8l/kube-proxy usage on node worker-02. CPUUsageInCores: 0.000125175, MemoryUsageInBytes: 60850176, MemoryWorkingSetInBytes: 31571968
I0319 05:53:18.603635 1729288 resource_gather_worker.go:68] Get container calico-node-mkshg/calico-node usage on node worker-02. CPUUsageInCores: 0.136779764, MemoryUsageInBytes: 91480064, MemoryWorkingSetInBytes: 69767168
I0319 05:53:18.762147 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-19): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:18.762358 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-19)
I0319 05:53:18.962486 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:19.162879 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-21): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:19.163277 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-21)
I0319 05:53:19.352971 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:19.558220 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-23): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:19.764079 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:19.951425 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-0): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:19.951629 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-0)
I0319 05:53:19.959125 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:20.148541 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-1): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:20.148868 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-1)
I0319 05:53:20.163023 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:20.346663 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-2): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:20.346951 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-2)
I0319 05:53:20.360043 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:20.561387 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-28): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:20.747556 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-4): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:20.747977 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-4)
I0319 05:53:20.761103 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:20.949794 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-5): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:20.950134 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-5)
I0319 05:53:21.155671 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-6): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:21.156037 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-6)
I0319 05:53:21.445344 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-03/kube-scheduler usage on node master-03. CPUUsageInCores: 0.015517017, MemoryUsageInBytes: 77541376, MemoryWorkingSetInBytes: 38236160
I0319 05:53:21.445391 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-03/kube-controller-manager usage on node master-03. CPUUsageInCores: 0.064586253, MemoryUsageInBytes: 184307712, MemoryWorkingSetInBytes: 109617152
I0319 05:53:21.445406 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-03/kube-apiserver usage on node master-03. CPUUsageInCores: 0.293826521, MemoryUsageInBytes: 1306693632, MemoryWorkingSetInBytes: 641449984
I0319 05:53:21.445417 1729288 resource_gather_worker.go:68] Get container kube-proxy-lksgf/kube-proxy usage on node master-03. CPUUsageInCores: 0.008393422, MemoryUsageInBytes: 54362112, MemoryWorkingSetInBytes: 27295744
I0319 05:53:21.445427 1729288 resource_gather_worker.go:68] Get container etcd-master-03/etcd usage on node master-03. CPUUsageInCores: 0.148416006, MemoryUsageInBytes: 693948416, MemoryWorkingSetInBytes: 184078336
I0319 05:53:21.445437 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-f2d6s/coredns usage on node master-03. CPUUsageInCores: 0.004690537, MemoryUsageInBytes: 25329664, MemoryWorkingSetInBytes: 10596352
I0319 05:53:21.445446 1729288 resource_gather_worker.go:68] Get container calico-node-kl7lj/calico-node usage on node master-03. CPUUsageInCores: 0.054237699, MemoryUsageInBytes: 68780032, MemoryWorkingSetInBytes: 56664064
I0319 05:53:21.951544 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-10): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:21.951802 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-10)
I0319 05:53:22.150906 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-11): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:22.151259 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-11)
I0319 05:53:22.357347 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-12): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:22.357483 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-12)
I0319 05:53:22.558194 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-13): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:22.558500 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-13)
I0319 05:53:22.910464 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-02/kube-scheduler usage on node master-02. CPUUsageInCores: 0.01508464, MemoryUsageInBytes: 76029952, MemoryWorkingSetInBytes: 24637440
I0319 05:53:22.910509 1729288 resource_gather_worker.go:68] Get container kube-proxy-tj5hq/kube-proxy usage on node master-02. CPUUsageInCores: 9.2966e-05, MemoryUsageInBytes: 53456896, MemoryWorkingSetInBytes: 25784320
I0319 05:53:22.910524 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-44j7w/coredns usage on node master-02. CPUUsageInCores: 0.003662164, MemoryUsageInBytes: 32944128, MemoryWorkingSetInBytes: 9695232
I0319 05:53:22.910535 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-02/kube-controller-manager usage on node master-02. CPUUsageInCores: 0.002384095, MemoryUsageInBytes: 48291840, MemoryWorkingSetInBytes: 25018368
I0319 05:53:22.910549 1729288 resource_gather_worker.go:68] Get container etcd-master-02/etcd usage on node master-02. CPUUsageInCores: 0.16245263, MemoryUsageInBytes: 706510848, MemoryWorkingSetInBytes: 133120000
I0319 05:53:22.910579 1729288 resource_gather_worker.go:68] Get container calico-node-2dd2z/calico-node usage on node master-02. CPUUsageInCores: 0.051441088, MemoryUsageInBytes: 69275648, MemoryWorkingSetInBytes: 54136832
I0319 05:53:22.910591 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-02/kube-apiserver usage on node master-02. CPUUsageInCores: 0.170022052, MemoryUsageInBytes: 1107517440, MemoryWorkingSetInBytes: 558657536
I0319 05:53:23.353084 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-17): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:23.353416 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-17)
I0319 05:53:23.558964 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-18): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:23.559203 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-18)
I0319 05:53:23.963273 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:24.353730 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:24.403237 1729288 resource_gather_worker.go:68] Get container kube-proxy-wf64w/kube-proxy usage on node master-01. CPUUsageInCores: 0.007647897, MemoryUsageInBytes: 54038528, MemoryWorkingSetInBytes: 27770880
I0319 05:53:24.403400 1729288 resource_gather_worker.go:68] Get container etcd-master-01/etcd usage on node master-01. CPUUsageInCores: 0.157710756, MemoryUsageInBytes: 653201408, MemoryWorkingSetInBytes: 163684352
I0319 05:53:24.403477 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-ttms9/coredns usage on node master-01. CPUUsageInCores: 0.003852421, MemoryUsageInBytes: 33345536, MemoryWorkingSetInBytes: 10637312
I0319 05:53:24.403595 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-01/kube-apiserver usage on node master-01. CPUUsageInCores: 0.196637368, MemoryUsageInBytes: 999104512, MemoryWorkingSetInBytes: 567635968
I0319 05:53:24.403664 1729288 resource_gather_worker.go:68] Get container calico-node-jp8v5/calico-node usage on node master-01. CPUUsageInCores: 0.04616231, MemoryUsageInBytes: 71610368, MemoryWorkingSetInBytes: 57053184
I0319 05:53:24.403766 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-01/kube-controller-manager usage on node master-01. CPUUsageInCores: 0.002251948, MemoryUsageInBytes: 56381440, MemoryWorkingSetInBytes: 25542656
I0319 05:53:24.403905 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-01/kube-scheduler usage on node master-01. CPUUsageInCores: 0.01710899, MemoryUsageInBytes: 89276416, MemoryWorkingSetInBytes: 41963520
I0319 05:53:24.559229 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-23): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:24.559449 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-23)
I0319 05:53:24.765030 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:24.960040 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:25.164263 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:25.164643 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-26)
I0319 05:53:25.360424 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:25.562710 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-28): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:25.562947 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-28)
I0319 05:53:25.762260 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 1 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:27.164102 1729288 resource_gather_worker.go:68] Get container calico-node-dfmlr/calico-node usage on node worker-03. CPUUsageInCores: 0.107398059, MemoryUsageInBytes: 98562048, MemoryWorkingSetInBytes: 79626240
I0319 05:53:27.164164 1729288 resource_gather_worker.go:68] Get container kube-proxy-5gd66/kube-proxy usage on node worker-03. CPUUsageInCores: 0.00046653, MemoryUsageInBytes: 48029696, MemoryWorkingSetInBytes: 32669696
I0319 05:53:28.964444 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:28.964675 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-20)
I0319 05:53:29.354869 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:29.355065 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-22)
I0319 05:53:29.766138 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:29.766338 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-24)
I0319 05:53:29.961229 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:29.961393 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-25)
I0319 05:53:30.361544 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:30.361647 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-27)
I0319 05:53:30.762662 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:30.762780 1729288 wait_for_controlled_pods.go:246] WaitForControlledPodsRunning: running 0, deleted 30, timeout: 0, unknown: 0
I0319 05:53:30.762813 1729288 wait_for_controlled_pods.go:260] WaitForControlledPodsRunning: 0/0 Deployments are running with all pods
I0319 05:53:30.762844 1729288 simple_test_executor.go:165] Step "[step: 10] Waiting for latency pods to be deleted" ended
I0319 05:53:30.762869 1729288 simple_test_executor.go:139] Step "[step: 11] Collecting pod startup latency" started
I0319 05:53:30.762931 1729288 pod_startup_latency.go:130] PodStartupLatency: labelSelector(group = latency): gathering pod startup latency measurement...
I0319 05:53:30.763131 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=latency-deployment-29)
I0319 05:53:30.763400 1729288 reflector.go:181] Stopping reflector <unspecified> (0s) from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:53:30.787134 1729288 phase_latency.go:130] PodStartupLatency: 30 worst create_to_schedule latencies: [{test-irdrhg-1/latency-deployment-2-56d949dbc5-h2mxq 34.749716ms} {test-irdrhg-1/latency-deployment-12-879cdf487-n7slw 38.35488ms} {test-irdrhg-1/latency-deployment-16-8c6b874bc-zbdzb 47.359012ms} {test-irdrhg-1/latency-deployment-7-85459d6b59-hkw6s 55.61842ms} {test-irdrhg-1/latency-deployment-27-7dcbd5ccf5-dfmvt 140.255819ms} {test-irdrhg-1/latency-deployment-20-69567f64df-5khsj 141.841349ms} {test-irdrhg-1/latency-deployment-3-7bc4d54b7f-xzgn2 236.20228ms} {test-irdrhg-1/latency-deployment-13-ff78b9b7c-2q2dc 243.606ms} {test-irdrhg-1/latency-deployment-8-679ffbd87-4pf6r 251.354963ms} {test-irdrhg-1/latency-deployment-17-779cdf4977-fc9j9 291.520141ms} {test-irdrhg-1/latency-deployment-24-6b5b748f-zlwqr 291.93402ms} {test-irdrhg-1/latency-deployment-21-8d9f68dc5-w67dq 438.877987ms} {test-irdrhg-1/latency-deployment-4-5548966b47-6pkvk 442.490228ms} {test-irdrhg-1/latency-deployment-14-7476ff5d67-txzct 448.854304ms} {test-irdrhg-1/latency-deployment-9-557dbd7d55-mrxt2 449.213148ms} {test-irdrhg-1/latency-deployment-18-5db5545d67-8bcnl 587.986816ms} {test-irdrhg-1/latency-deployment-28-647c5b7f57-tlmrj 640.864972ms} {test-irdrhg-1/latency-deployment-0-6dfbf769c5-kjn8k 645.783141ms} {test-irdrhg-1/latency-deployment-10-55dd65cd67-8fflf 647.011783ms} {test-irdrhg-1/latency-deployment-5-54b5578d95-st7lg 652.719617ms} {test-irdrhg-1/latency-deployment-25-768bf98747-k2hjw 694.195579ms} {test-irdrhg-1/latency-deployment-15-58f9d5f985-trvrg 788.706451ms} {test-irdrhg-1/latency-deployment-29-8c4d9d8c-s72qw 796.361473ms} {test-irdrhg-1/latency-deployment-11-cc97bd98f-w9ttx 838.214406ms} {test-irdrhg-1/latency-deployment-1-85794469b7-8r6c8 843.421402ms} {test-irdrhg-1/latency-deployment-6-65bcfdc9c9-h7zdl 849.352851ms} {test-irdrhg-1/latency-deployment-22-55d98f89b9-sjsqs 890.498698ms} {test-irdrhg-1/latency-deployment-23-5b95bdff6c-d7kkt 941.862179ms} {test-irdrhg-1/latency-deployment-19-5d9bb47745-c26j7 989.483651ms} {test-irdrhg-1/latency-deployment-26-54b7bcd857-hc8g9 992.622311ms}]
I0319 05:53:30.787259 1729288 phase_latency.go:135] PodStartupLatency: perc50: 449.213148ms, perc90: 890.498698ms, perc99: 992.622311ms
I0319 05:53:30.787307 1729288 phase_latency.go:130] PodStartupLatency: 30 worst schedule_to_run latencies: [{test-irdrhg-1/latency-deployment-21-8d9f68dc5-w67dq 561.122013ms} {test-irdrhg-1/latency-deployment-8-679ffbd87-4pf6r 748.645037ms} {test-irdrhg-1/latency-deployment-20-69567f64df-5khsj 858.158651ms} {test-irdrhg-1/latency-deployment-7-85459d6b59-hkw6s 944.38158ms} {test-irdrhg-1/latency-deployment-2-56d949dbc5-h2mxq 965.250284ms} {test-irdrhg-1/latency-deployment-19-5d9bb47745-c26j7 1.010516349s} {test-irdrhg-1/latency-deployment-6-65bcfdc9c9-h7zdl 1.150647149s} {test-irdrhg-1/latency-deployment-1-85794469b7-8r6c8 1.156578598s} {test-irdrhg-1/latency-deployment-29-8c4d9d8c-s72qw 1.203638527s} {test-irdrhg-1/latency-deployment-15-58f9d5f985-trvrg 1.211293549s} {test-irdrhg-1/latency-deployment-25-768bf98747-k2hjw 1.305804421s} {test-irdrhg-1/latency-deployment-5-54b5578d95-st7lg 1.347280383s} {test-irdrhg-1/latency-deployment-10-55dd65cd67-8fflf 1.352988217s} {test-irdrhg-1/latency-deployment-0-6dfbf769c5-kjn8k 1.354216859s} {test-irdrhg-1/latency-deployment-28-647c5b7f57-tlmrj 1.359135028s} {test-irdrhg-1/latency-deployment-18-5db5545d67-8bcnl 1.412013184s} {test-irdrhg-1/latency-deployment-9-557dbd7d55-mrxt2 1.550786852s} {test-irdrhg-1/latency-deployment-14-7476ff5d67-txzct 1.551145696s} {test-irdrhg-1/latency-deployment-4-5548966b47-6pkvk 1.557509772s} {test-irdrhg-1/latency-deployment-24-6b5b748f-zlwqr 1.70806598s} {test-irdrhg-1/latency-deployment-17-779cdf4977-fc9j9 1.708479859s} {test-irdrhg-1/latency-deployment-13-ff78b9b7c-2q2dc 1.756394s} {test-irdrhg-1/latency-deployment-3-7bc4d54b7f-xzgn2 1.76379772s} {test-irdrhg-1/latency-deployment-27-7dcbd5ccf5-dfmvt 1.859744181s} {test-irdrhg-1/latency-deployment-16-8c6b874bc-zbdzb 1.952640988s} {test-irdrhg-1/latency-deployment-12-879cdf487-n7slw 1.96164512s} {test-irdrhg-1/latency-deployment-26-54b7bcd857-hc8g9 2.007377689s} {test-irdrhg-1/latency-deployment-23-5b95bdff6c-d7kkt 2.058137821s} {test-irdrhg-1/latency-deployment-22-55d98f89b9-sjsqs 2.109501302s} {test-irdrhg-1/latency-deployment-11-cc97bd98f-w9ttx 2.161785594s}]
I0319 05:53:30.787623 1729288 phase_latency.go:135] PodStartupLatency: perc50: 1.359135028s, perc90: 2.007377689s, perc99: 2.161785594s
I0319 05:53:30.787792 1729288 phase_latency.go:130] PodStartupLatency: 30 worst run_to_watch latencies: [{test-irdrhg-1/latency-deployment-16-8c6b874bc-zbdzb 466.73387ms} {test-irdrhg-1/latency-deployment-3-7bc4d54b7f-xzgn2 508.671487ms} {test-irdrhg-1/latency-deployment-11-cc97bd98f-w9ttx 666.986362ms} {test-irdrhg-1/latency-deployment-19-5d9bb47745-c26j7 798.004705ms} {test-irdrhg-1/latency-deployment-24-6b5b748f-zlwqr 955.713686ms} {test-irdrhg-1/latency-deployment-22-55d98f89b9-sjsqs 984.663074ms} {test-irdrhg-1/latency-deployment-25-768bf98747-k2hjw 1.156569735s} {test-irdrhg-1/latency-deployment-27-7dcbd5ccf5-dfmvt 1.156756871s} {test-irdrhg-1/latency-deployment-10-55dd65cd67-8fflf 1.171437764s} {test-irdrhg-1/latency-deployment-21-8d9f68dc5-w67dq 1.202506968s} {test-irdrhg-1/latency-deployment-4-5548966b47-6pkvk 1.264825488s} {test-irdrhg-1/latency-deployment-14-7476ff5d67-txzct 1.268470305s} {test-irdrhg-1/latency-deployment-9-557dbd7d55-mrxt2 1.271640768s} {test-irdrhg-1/latency-deployment-17-779cdf4977-fc9j9 1.272780519s} {test-irdrhg-1/latency-deployment-20-69567f64df-5khsj 1.344952909s} {test-irdrhg-1/latency-deployment-29-8c4d9d8c-s72qw 1.364128344s} {test-irdrhg-1/latency-deployment-2-56d949dbc5-h2mxq 1.467467654s} {test-irdrhg-1/latency-deployment-1-85794469b7-8r6c8 1.481786292s} {test-irdrhg-1/latency-deployment-23-5b95bdff6c-d7kkt 1.559067947s} {test-irdrhg-1/latency-deployment-7-85459d6b59-hkw6s 1.663594274s} {test-irdrhg-1/latency-deployment-28-647c5b7f57-tlmrj 1.761279289s} {test-irdrhg-1/latency-deployment-0-6dfbf769c5-kjn8k 1.862627292s} {test-irdrhg-1/latency-deployment-12-879cdf487-n7slw 1.871256906s} {test-irdrhg-1/latency-deployment-5-54b5578d95-st7lg 2.061403353s} {test-irdrhg-1/latency-deployment-26-54b7bcd857-hc8g9 2.155407214s} {test-irdrhg-1/latency-deployment-6-65bcfdc9c9-h7zdl 2.461315136s} {test-irdrhg-1/latency-deployment-18-5db5545d67-8bcnl 2.472970529s} {test-irdrhg-1/latency-deployment-13-ff78b9b7c-2q2dc 2.666264648s} {test-irdrhg-1/latency-deployment-8-679ffbd87-4pf6r 2.86497222s} {test-irdrhg-1/latency-deployment-15-58f9d5f985-trvrg 3.073008179s}]
I0319 05:53:30.787876 1729288 phase_latency.go:135] PodStartupLatency: perc50: 1.344952909s, perc90: 2.472970529s, perc99: 3.073008179s
I0319 05:53:30.787916 1729288 phase_latency.go:130] PodStartupLatency: 30 worst schedule_to_watch latencies: [{test-irdrhg-1/latency-deployment-21-8d9f68dc5-w67dq 1.763628981s} {test-irdrhg-1/latency-deployment-19-5d9bb47745-c26j7 1.808521054s} {test-irdrhg-1/latency-deployment-20-69567f64df-5khsj 2.20311156s} {test-irdrhg-1/latency-deployment-3-7bc4d54b7f-xzgn2 2.272469207s} {test-irdrhg-1/latency-deployment-16-8c6b874bc-zbdzb 2.419374858s} {test-irdrhg-1/latency-deployment-2-56d949dbc5-h2mxq 2.432717938s} {test-irdrhg-1/latency-deployment-25-768bf98747-k2hjw 2.462374156s} {test-irdrhg-1/latency-deployment-10-55dd65cd67-8fflf 2.524425981s} {test-irdrhg-1/latency-deployment-29-8c4d9d8c-s72qw 2.567766871s} {test-irdrhg-1/latency-deployment-7-85459d6b59-hkw6s 2.607975854s} {test-irdrhg-1/latency-deployment-1-85794469b7-8r6c8 2.63836489s} {test-irdrhg-1/latency-deployment-24-6b5b748f-zlwqr 2.663779666s} {test-irdrhg-1/latency-deployment-14-7476ff5d67-txzct 2.819616001s} {test-irdrhg-1/latency-deployment-4-5548966b47-6pkvk 2.82233526s} {test-irdrhg-1/latency-deployment-9-557dbd7d55-mrxt2 2.82242762s} {test-irdrhg-1/latency-deployment-11-cc97bd98f-w9ttx 2.828771956s} {test-irdrhg-1/latency-deployment-17-779cdf4977-fc9j9 2.981260378s} {test-irdrhg-1/latency-deployment-27-7dcbd5ccf5-dfmvt 3.016501052s} {test-irdrhg-1/latency-deployment-22-55d98f89b9-sjsqs 3.094164376s} {test-irdrhg-1/latency-deployment-28-647c5b7f57-tlmrj 3.120414317s} {test-irdrhg-1/latency-deployment-0-6dfbf769c5-kjn8k 3.216844151s} {test-irdrhg-1/latency-deployment-5-54b5578d95-st7lg 3.408683736s} {test-irdrhg-1/latency-deployment-6-65bcfdc9c9-h7zdl 3.611962285s} {test-irdrhg-1/latency-deployment-8-679ffbd87-4pf6r 3.613617257s} {test-irdrhg-1/latency-deployment-23-5b95bdff6c-d7kkt 3.617205768s} {test-irdrhg-1/latency-deployment-12-879cdf487-n7slw 3.832902026s} {test-irdrhg-1/latency-deployment-18-5db5545d67-8bcnl 3.884983713s} {test-irdrhg-1/latency-deployment-26-54b7bcd857-hc8g9 4.162784903s} {test-irdrhg-1/latency-deployment-15-58f9d5f985-trvrg 4.284301728s} {test-irdrhg-1/latency-deployment-13-ff78b9b7c-2q2dc 4.422658648s}]
I0319 05:53:30.788136 1729288 phase_latency.go:135] PodStartupLatency: perc50: 2.82242762s, perc90: 3.884983713s, perc99: 4.422658648s
I0319 05:53:30.788176 1729288 phase_latency.go:130] PodStartupLatency: 30 worst pod_startup latencies: [{test-irdrhg-1/latency-deployment-21-8d9f68dc5-w67dq 2.202506968s} {test-irdrhg-1/latency-deployment-20-69567f64df-5khsj 2.344952909s} {test-irdrhg-1/latency-deployment-16-8c6b874bc-zbdzb 2.46673387s} {test-irdrhg-1/latency-deployment-2-56d949dbc5-h2mxq 2.467467654s} {test-irdrhg-1/latency-deployment-3-7bc4d54b7f-xzgn2 2.508671487s} {test-irdrhg-1/latency-deployment-7-85459d6b59-hkw6s 2.663594274s} {test-irdrhg-1/latency-deployment-19-5d9bb47745-c26j7 2.798004705s} {test-irdrhg-1/latency-deployment-24-6b5b748f-zlwqr 2.955713686s} {test-irdrhg-1/latency-deployment-25-768bf98747-k2hjw 3.156569735s} {test-irdrhg-1/latency-deployment-27-7dcbd5ccf5-dfmvt 3.156756871s} {test-irdrhg-1/latency-deployment-10-55dd65cd67-8fflf 3.171437764s} {test-irdrhg-1/latency-deployment-4-5548966b47-6pkvk 3.264825488s} {test-irdrhg-1/latency-deployment-14-7476ff5d67-txzct 3.268470305s} {test-irdrhg-1/latency-deployment-9-557dbd7d55-mrxt2 3.271640768s} {test-irdrhg-1/latency-deployment-17-779cdf4977-fc9j9 3.272780519s} {test-irdrhg-1/latency-deployment-29-8c4d9d8c-s72qw 3.364128344s} {test-irdrhg-1/latency-deployment-1-85794469b7-8r6c8 3.481786292s} {test-irdrhg-1/latency-deployment-11-cc97bd98f-w9ttx 3.666986362s} {test-irdrhg-1/latency-deployment-28-647c5b7f57-tlmrj 3.761279289s} {test-irdrhg-1/latency-deployment-0-6dfbf769c5-kjn8k 3.862627292s} {test-irdrhg-1/latency-deployment-8-679ffbd87-4pf6r 3.86497222s} {test-irdrhg-1/latency-deployment-12-879cdf487-n7slw 3.871256906s} {test-irdrhg-1/latency-deployment-22-55d98f89b9-sjsqs 3.984663074s} {test-irdrhg-1/latency-deployment-5-54b5578d95-st7lg 4.061403353s} {test-irdrhg-1/latency-deployment-6-65bcfdc9c9-h7zdl 4.461315136s} {test-irdrhg-1/latency-deployment-18-5db5545d67-8bcnl 4.472970529s} {test-irdrhg-1/latency-deployment-23-5b95bdff6c-d7kkt 4.559067947s} {test-irdrhg-1/latency-deployment-13-ff78b9b7c-2q2dc 4.666264648s} {test-irdrhg-1/latency-deployment-15-58f9d5f985-trvrg 5.073008179s} {test-irdrhg-1/latency-deployment-26-54b7bcd857-hc8g9 5.155407214s}]
I0319 05:53:30.788395 1729288 phase_latency.go:135] PodStartupLatency: perc50: 3.272780519s, perc90: 4.559067947s, perc99: 5.155407214s; threshold 5s
E0319 05:53:30.788577 1729288 pod_startup_latency.go:168] PodStartupLatency: labelSelector(group = latency): pod startup: too high latency 99th percentile: got 5.155407214s expected: 5s
I0319 05:53:30.788908 1729288 simple_test_executor.go:165] Step "[step: 11] Collecting pod startup latency" ended
W0319 05:53:30.788937 1729288 simple_test_executor.go:167] Got errors during step execution: [measurement call PodStartupLatency - PodStartupLatency error: pod startup: too high latency 99th percentile: got 5.155407214s expected: 5s]
I0319 05:53:30.789084 1729288 simple_test_executor.go:139] Step "[step: 12] Deleting saturation pods" started
I0319 05:53:30.815640 1729288 reflector.go:175] Starting reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0)
I0319 05:53:30.815677 1729288 reflector.go:211] Listing and watching *v1.Pod from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0)
I0319 05:53:30.994941 1729288 simple_test_executor.go:165] Step "[step: 12] Deleting saturation pods" ended
I0319 05:53:30.994997 1729288 simple_test_executor.go:139] Step "[step: 13] Waiting for saturation pods to be deleted" started
I0319 05:53:30.995072 1729288 wait_for_controlled_pods.go:188] WaitForControlledPodsRunning: waiting for controlled pods measurement...
I0319 05:53:31.471723 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-03/kube-apiserver usage on node master-03. CPUUsageInCores: 0.213531885, MemoryUsageInBytes: 1306697728, MemoryWorkingSetInBytes: 641454080
I0319 05:53:31.472133 1729288 resource_gather_worker.go:68] Get container calico-node-kl7lj/calico-node usage on node master-03. CPUUsageInCores: 0.053718423, MemoryUsageInBytes: 70352896, MemoryWorkingSetInBytes: 58236928
I0319 05:53:31.472369 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-f2d6s/coredns usage on node master-03. CPUUsageInCores: 0.009109244, MemoryUsageInBytes: 25329664, MemoryWorkingSetInBytes: 10596352
I0319 05:53:31.472540 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-03/kube-controller-manager usage on node master-03. CPUUsageInCores: 0.030151344, MemoryUsageInBytes: 184307712, MemoryWorkingSetInBytes: 109617152
I0319 05:53:31.472678 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-03/kube-scheduler usage on node master-03. CPUUsageInCores: 0.00967993, MemoryUsageInBytes: 77541376, MemoryWorkingSetInBytes: 38236160
I0319 05:53:31.472836 1729288 resource_gather_worker.go:68] Get container kube-proxy-lksgf/kube-proxy usage on node master-03. CPUUsageInCores: 0.008393422, MemoryUsageInBytes: 54362112, MemoryWorkingSetInBytes: 27295744
I0319 05:53:31.472964 1729288 resource_gather_worker.go:68] Get container etcd-master-03/etcd usage on node master-03. CPUUsageInCores: 0.143515163, MemoryUsageInBytes: 694202368, MemoryWorkingSetInBytes: 183656448
I0319 05:53:32.934932 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-44j7w/coredns usage on node master-02. CPUUsageInCores: 0.003991195, MemoryUsageInBytes: 32944128, MemoryWorkingSetInBytes: 9695232
I0319 05:53:32.934998 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-02/kube-apiserver usage on node master-02. CPUUsageInCores: 0.102441166, MemoryUsageInBytes: 1107517440, MemoryWorkingSetInBytes: 558657536
I0319 05:53:32.935008 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-02/kube-controller-manager usage on node master-02. CPUUsageInCores: 0.002849968, MemoryUsageInBytes: 48291840, MemoryWorkingSetInBytes: 25018368
I0319 05:53:32.935015 1729288 resource_gather_worker.go:68] Get container etcd-master-02/etcd usage on node master-02. CPUUsageInCores: 0.16245263, MemoryUsageInBytes: 706510848, MemoryWorkingSetInBytes: 133120000
I0319 05:53:32.935021 1729288 resource_gather_worker.go:68] Get container calico-node-2dd2z/calico-node usage on node master-02. CPUUsageInCores: 0.054205221, MemoryUsageInBytes: 69275648, MemoryWorkingSetInBytes: 54136832
I0319 05:53:32.935027 1729288 resource_gather_worker.go:68] Get container kube-proxy-tj5hq/kube-proxy usage on node master-02. CPUUsageInCores: 0.006543429, MemoryUsageInBytes: 53473280, MemoryWorkingSetInBytes: 25800704
I0319 05:53:32.935034 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-02/kube-scheduler usage on node master-02. CPUUsageInCores: 0.007216784, MemoryUsageInBytes: 76029952, MemoryWorkingSetInBytes: 24637440
I0319 05:53:34.431303 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-01/kube-apiserver usage on node master-01. CPUUsageInCores: 0.163743273, MemoryUsageInBytes: 999104512, MemoryWorkingSetInBytes: 567635968
I0319 05:53:34.434300 1729288 resource_gather_worker.go:68] Get container kube-proxy-wf64w/kube-proxy usage on node master-01. CPUUsageInCores: 0.007647897, MemoryUsageInBytes: 54038528, MemoryWorkingSetInBytes: 27770880
I0319 05:53:34.434340 1729288 resource_gather_worker.go:68] Get container etcd-master-01/etcd usage on node master-01. CPUUsageInCores: 0.157710756, MemoryUsageInBytes: 653201408, MemoryWorkingSetInBytes: 163684352
I0319 05:53:34.434353 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-01/kube-scheduler usage on node master-01. CPUUsageInCores: 0.0086768, MemoryUsageInBytes: 89276416, MemoryWorkingSetInBytes: 41963520
I0319 05:53:34.434385 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-ttms9/coredns usage on node master-01. CPUUsageInCores: 0.004579343, MemoryUsageInBytes: 33345536, MemoryWorkingSetInBytes: 10637312
I0319 05:53:34.434397 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-01/kube-controller-manager usage on node master-01. CPUUsageInCores: 0.002251948, MemoryUsageInBytes: 56381440, MemoryWorkingSetInBytes: 25542656
I0319 05:53:34.434422 1729288 resource_gather_worker.go:68] Get container calico-node-jp8v5/calico-node usage on node master-01. CPUUsageInCores: 0.058625131, MemoryUsageInBytes: 71581696, MemoryWorkingSetInBytes: 57024512
I0319 05:53:35.708042 1729288 resource_gather_worker.go:68] Get container kube-proxy-rt66n/kube-proxy usage on node worker-04. CPUUsageInCores: 0.007782695, MemoryUsageInBytes: 40751104, MemoryWorkingSetInBytes: 25341952
I0319 05:53:35.708092 1729288 resource_gather_worker.go:68] Get container calico-kube-controllers-5bc44b965b-sz92b/calico-kube-controllers usage on node worker-04. CPUUsageInCores: 0.004606322, MemoryUsageInBytes: 20688896, MemoryWorkingSetInBytes: 15958016
I0319 05:53:35.708104 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-9pc6k/coredns usage on node worker-04. CPUUsageInCores: 0.00488734, MemoryUsageInBytes: 56668160, MemoryWorkingSetInBytes: 11927552
I0319 05:53:35.708128 1729288 resource_gather_worker.go:68] Get container calico-node-bmrwr/calico-node usage on node worker-04. CPUUsageInCores: 0.072512867, MemoryUsageInBytes: 91820032, MemoryWorkingSetInBytes: 68165632
I0319 05:53:35.708138 1729288 resource_gather_worker.go:68] Get container metrics-server-7c6589f8ff-kkpbz/metrics-server usage on node worker-04. CPUUsageInCores: 0.01109826, MemoryUsageInBytes: 33849344, MemoryWorkingSetInBytes: 24252416
I0319 05:53:35.867038 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 29 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:40.868199 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 3 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:41.499025 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-03/kube-controller-manager usage on node master-03. CPUUsageInCores: 0.030151344, MemoryUsageInBytes: 184307712, MemoryWorkingSetInBytes: 109617152
I0319 05:53:41.499071 1729288 resource_gather_worker.go:68] Get container kube-proxy-lksgf/kube-proxy usage on node master-03. CPUUsageInCores: 0.001067169, MemoryUsageInBytes: 54362112, MemoryWorkingSetInBytes: 27295744
I0319 05:53:41.499078 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-03/kube-scheduler usage on node master-03. CPUUsageInCores: 0.014220616, MemoryUsageInBytes: 77541376, MemoryWorkingSetInBytes: 38236160
I0319 05:53:41.499084 1729288 resource_gather_worker.go:68] Get container etcd-master-03/etcd usage on node master-03. CPUUsageInCores: 0.143515163, MemoryUsageInBytes: 694202368, MemoryWorkingSetInBytes: 183656448
I0319 05:53:41.499091 1729288 resource_gather_worker.go:68] Get container calico-node-kl7lj/calico-node usage on node master-03. CPUUsageInCores: 0.057638904, MemoryUsageInBytes: 68784128, MemoryWorkingSetInBytes: 56803328
I0319 05:53:41.499096 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-03/kube-apiserver usage on node master-03. CPUUsageInCores: 0.213531885, MemoryUsageInBytes: 1306697728, MemoryWorkingSetInBytes: 641454080
I0319 05:53:41.499102 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-f2d6s/coredns usage on node master-03. CPUUsageInCores: 0.009109244, MemoryUsageInBytes: 25329664, MemoryWorkingSetInBytes: 10596352
I0319 05:53:42.959967 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-02/kube-scheduler usage on node master-02. CPUUsageInCores: 0.007216784, MemoryUsageInBytes: 76029952, MemoryWorkingSetInBytes: 24637440
I0319 05:53:42.960027 1729288 resource_gather_worker.go:68] Get container etcd-master-02/etcd usage on node master-02. CPUUsageInCores: 0.128257959, MemoryUsageInBytes: 706912256, MemoryWorkingSetInBytes: 132845568
I0319 05:53:42.960039 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-02/kube-apiserver usage on node master-02. CPUUsageInCores: 0.162357924, MemoryUsageInBytes: 1107525632, MemoryWorkingSetInBytes: 558665728
I0319 05:53:42.960049 1729288 resource_gather_worker.go:68] Get container kube-proxy-tj5hq/kube-proxy usage on node master-02. CPUUsageInCores: 0.00012283, MemoryUsageInBytes: 53473280, MemoryWorkingSetInBytes: 25800704
I0319 05:53:42.960060 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-02/kube-controller-manager usage on node master-02. CPUUsageInCores: 0.002849968, MemoryUsageInBytes: 48291840, MemoryWorkingSetInBytes: 25018368
I0319 05:53:42.960070 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-44j7w/coredns usage on node master-02. CPUUsageInCores: 0.003991195, MemoryUsageInBytes: 32944128, MemoryWorkingSetInBytes: 9695232
I0319 05:53:42.960079 1729288 resource_gather_worker.go:68] Get container calico-node-2dd2z/calico-node usage on node master-02. CPUUsageInCores: 0.052632679, MemoryUsageInBytes: 69275648, MemoryWorkingSetInBytes: 54136832
I0319 05:53:44.460552 1729288 resource_gather_worker.go:68] Get container etcd-master-01/etcd usage on node master-01. CPUUsageInCores: 0.119568664, MemoryUsageInBytes: 653832192, MemoryWorkingSetInBytes: 163774464
I0319 05:53:44.460669 1729288 resource_gather_worker.go:68] Get container kube-scheduler-master-01/kube-scheduler usage on node master-01. CPUUsageInCores: 0.0086768, MemoryUsageInBytes: 89276416, MemoryWorkingSetInBytes: 41963520
I0319 05:53:44.460700 1729288 resource_gather_worker.go:68] Get container kube-controller-manager-master-01/kube-controller-manager usage on node master-01. CPUUsageInCores: 0.003265061, MemoryUsageInBytes: 56381440, MemoryWorkingSetInBytes: 25542656
I0319 05:53:44.460724 1729288 resource_gather_worker.go:68] Get container kube-proxy-wf64w/kube-proxy usage on node master-01. CPUUsageInCores: 0.000150295, MemoryUsageInBytes: 54038528, MemoryWorkingSetInBytes: 27770880
I0319 05:53:44.460760 1729288 resource_gather_worker.go:68] Get container calico-node-jp8v5/calico-node usage on node master-01. CPUUsageInCores: 0.058625131, MemoryUsageInBytes: 71581696, MemoryWorkingSetInBytes: 57024512
I0319 05:53:44.460786 1729288 resource_gather_worker.go:68] Get container coredns-d9cc88495-ttms9/coredns usage on node master-01. CPUUsageInCores: 0.005046774, MemoryUsageInBytes: 33345536, MemoryWorkingSetInBytes: 10637312
I0319 05:53:44.460809 1729288 resource_gather_worker.go:68] Get container kube-apiserver-master-01/kube-apiserver usage on node master-01. CPUUsageInCores: 0.163743273, MemoryUsageInBytes: 999104512, MemoryWorkingSetInBytes: 567635968
I0319 05:53:45.868750 1729288 wait_for_pods.go:90] WaitForControlledPodsRunning: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0): Pods: 0 out of 0 created, 0 running (0 updated), 0 pending scheduled, 0 not scheduled, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0319 05:53:45.868849 1729288 wait_for_controlled_pods.go:246] WaitForControlledPodsRunning: running 0, deleted 1, timeout: 0, unknown: 0
I0319 05:53:45.869525 1729288 wait_for_controlled_pods.go:260] WaitForControlledPodsRunning: 0/0 Deployments are running with all pods
I0319 05:53:45.869557 1729288 simple_test_executor.go:165] Step "[step: 13] Waiting for saturation pods to be deleted" ended
I0319 05:53:45.869584 1729288 simple_test_executor.go:139] Step "[step: 14] Collecting measurements" started
I0319 05:53:45.869161 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(test-irdrhg-1), labelSelector(name=saturation-deployment-0)
W0319 05:53:45.869673 1729288 prometheus_measurement.go:59] APIResponsivenessPrometheus: Prometheus is disabled, skipping the measurement!
W0319 05:53:45.869741 1729288 prometheus_measurement.go:59] APIResponsivenessPrometheusSimple: Prometheus is disabled, skipping the measurement!
W0319 05:53:45.869769 1729288 probes.go:106] InClusterNetworkLatency: Prometheus is disabled, skipping the measurement!
W0319 05:53:45.869792 1729288 probes.go:106] DnsLookupLatency: Prometheus is disabled, skipping the measurement!
I0319 05:53:47.181128 1729288 scheduler_latency.go:134] SchedulingMetrics: gathering latency metrics in scheduler...
W0319 05:53:47.231202 1729288 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
I0319 05:53:47.909559 1729288 resource_usage.go:130] ResourceUsageSummary: gathering resource usage...
I0319 05:53:47.909595 1729288 container_resource_gatherer.go:191] Closed stop channel. Waiting for 7 workers
I0319 05:53:47.909619 1729288 resource_gather_worker.go:87] Closing worker for master-01
I0319 05:53:47.909625 1729288 resource_gather_worker.go:87] Closing worker for master-02
I0319 05:53:47.909628 1729288 resource_gather_worker.go:87] Closing worker for master-03
I0319 05:53:47.909631 1729288 resource_gather_worker.go:87] Closing worker for worker-04
I0319 05:53:47.909635 1729288 resource_gather_worker.go:87] Closing worker for worker-03
I0319 05:53:47.909638 1729288 resource_gather_worker.go:87] Closing worker for worker-02
I0319 05:53:47.909642 1729288 resource_gather_worker.go:87] Closing worker for worker-01
I0319 05:53:47.909646 1729288 container_resource_gatherer.go:199] Waitgroup finished.
I0319 05:53:47.910193 1729288 system_pod_metrics.go:125] collecting system pod metrics...
I0319 05:53:47.924865 1729288 system_pod_metrics.go:229] Loaded restart count threshold overrides: map[]
I0319 05:53:47.925688 1729288 ooms_tracker.go:86] skipping tracking of OOMs in the cluster
I0319 05:53:47.925797 1729288 simple_test_executor.go:165] Step "[step: 14] Collecting measurements" ended
I0319 05:53:47.925869 1729288 simple_test_executor.go:72] Waiting for the chaos monkey subroutine to end...
I0319 05:53:47.925941 1729288 simple_test_executor.go:74] Chaos monkey ended.
I0319 05:53:47.943417 1729288 simple_test_executor.go:94] 
I0319 05:53:47.943454 1729288 probes.go:131] Probe InClusterNetworkLatency wasn't started, skipping the Dispose() step
I0319 05:53:47.943476 1729288 probes.go:131] Probe DnsLookupLatency wasn't started, skipping the Dispose() step
I0319 05:53:47.943683 1729288 reflector.go:181] Stopping reflector *unstructured.Unstructured (0s) from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:53:47.943894 1729288 reflector.go:181] Stopping reflector *unstructured.Unstructured (0s) from pkg/mod/k8s.io/client-go@v0.18.0/tools/cache/reflector.go:125
I0319 05:53:57.972432 1729288 simple_test_executor.go:387] Resources cleanup time: 10.028952061s
E0319 05:53:57.972597 1729288 clusterloader.go:218] --------------------------------------------------------------------------------
E0319 05:53:57.972664 1729288 clusterloader.go:219] Test Finished
E0319 05:53:57.972712 1729288 clusterloader.go:220]   Test: ./mode-3-10-30-reports-2021-03-19-0552/config.yaml
E0319 05:53:57.972757 1729288 clusterloader.go:221]   Status: Fail
E0319 05:53:57.972800 1729288 clusterloader.go:223]   Errors: [measurement call PodStartupLatency - PodStartupLatency error: pod startup: too high latency 99th percentile: got 5.155407214s expected: 5s]
E0319 05:53:57.972861 1729288 clusterloader.go:225] --------------------------------------------------------------------------------

JUnit report was created: /home/ubuntu/clusterloader2/mode-3-10-30-reports-2021-03-19-0552/junit.xml
I0319 05:53:57.973755 1729288 exec_service.go:107] Exec service: tearing down service
I0319 05:53:57.974154 1729288 reflector.go:181] Stopping reflector *v1.Pod (0s) from *v1.PodStore: namespace(cluster-loader), labelSelector(feature = exec)
F0319 05:54:33.009249 1729288 clusterloader.go:322] 2 tests have failed!
